import os
import json
import time
import asyncio
from typing import AsyncGenerator
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import signal
import sys

from config import Config
from llm_client import LLMClient
from retriever import PaperRetriever
from idea_generator import IdeaGenerator


def load_env_file(env_file: str):
    """åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶"""
    if not os.path.isabs(env_file):
        current_dir = os.path.dirname(os.path.abspath(__file__))
        env_file = os.path.join(current_dir, env_file)
    
    if os.path.exists(env_file):
        print(f"âœ“ æ‰¾åˆ° .env æ–‡ä»¶: {env_file}")
        loaded_count = 0
        with open(env_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key] = value.strip('"\'')  # å»é™¤å¼•å·
                    loaded_count += 1
        print(f"âœ“ æˆåŠŸåŠ è½½ {loaded_count} ä¸ªç¯å¢ƒå˜é‡")
        return True
    else:
        print(f"âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ° .env æ–‡ä»¶: {env_file}")
        return False


# åŠ è½½ç¯å¢ƒå˜é‡
load_env_file(".env")

# åˆ›å»ºFastAPIåº”ç”¨ - æ˜¾å¼æŒ‡å®šdocså’Œredocè·¯å¾„
app = FastAPI(
    title="ICAIS2025-Ideation API",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# ç§»é™¤å¯èƒ½å¯¼è‡´é˜»å¡çš„è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶
# ä½¿ç”¨æ›´ç®€å•çš„æ—¥å¿—è®°å½•æ–¹å¼

@app.middleware("http")
async def simple_log_middleware(request, call_next):
    """ç®€åŒ–çš„æ—¥å¿—ä¸­é—´ä»¶ï¼Œé¿å…é˜»å¡"""
    start_time = time.time()
    path = request.url.path
    
    # åªè®°å½•éå¥åº·æ£€æŸ¥çš„æ—¥å¿—ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
    if not path.startswith("/health"):
        print(f"ğŸ“¥ [{time.strftime('%H:%M:%S')}] {request.method} {path}")
    
    try:
        response = await call_next(request)
        process_time = time.time() - start_time
        if not path.startswith("/health"):
            print(f"ğŸ“¤ [{time.strftime('%H:%M:%S')}] {request.method} {path} - {response.status_code} ({process_time:.3f}s)")
        return response
    except Exception as e:
        print(f"âŒ [{time.strftime('%H:%M:%S')}] é”™è¯¯: {request.method} {path} - {e}")
        raise

# é…ç½®CORS - æ˜ç¡®è®¾ç½®originsè€Œä¸æ˜¯*
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

# è®¾ç½®å…¨å±€è¶…æ—¶
REQUEST_TIMEOUT = 600  # 10åˆ†é’Ÿè¶…æ—¶


class IdeationRequest(BaseModel):
    query: str


def format_sse_data(content: str) -> str:
    """ç”ŸæˆOpenAIæ ¼å¼çš„SSEæ•°æ®"""
    data = {
        "object": "chat.completion.chunk",
        "choices": [{
            "delta": {
                "content": content
            }
        }]
    }
    return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"

def format_sse_done() -> str:
    """ç”ŸæˆSSEç»“æŸæ ‡è®°"""
    return "data: [DONE]\n\n"

def stream_message(message: str, chunk_size: int = 1):
    """å°†æ¶ˆæ¯æŒ‰å­—ç¬¦æµå¼è¾“å‡ºï¼ˆåŒæ­¥ç”Ÿæˆå™¨ï¼‰"""
    for i in range(0, len(message), chunk_size):
        chunk = message[i:i + chunk_size]
        yield format_sse_data(chunk)


async def run_with_heartbeat(task_func, *args, heartbeat_interval=25, **kwargs):
    """
    æ‰§è¡Œé•¿æ—¶é—´ä»»åŠ¡ï¼ŒæœŸé—´å®šæœŸå‘é€å¿ƒè·³æ•°æ®
    
    Args:
        task_func: è¦æ‰§è¡Œçš„åŒæ­¥å‡½æ•°
        *args, **kwargs: ä¼ é€’ç»™å‡½æ•°çš„å‚æ•°
        heartbeat_interval: å¿ƒè·³é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤25ç§’
    
    Yields:
        å¿ƒè·³æ•°æ®ï¼ˆç©ºæ ¼å­—ç¬¦ï¼‰æˆ–ä»»åŠ¡ç»“æœ
    """
    import asyncio
    import time
    
    start_time = time.time()
    last_heartbeat = start_time
    
    # åˆ›å»ºä»»åŠ¡ï¼ˆä½¿ç”¨asyncio.to_threadå°†åŒæ­¥å‡½æ•°è½¬æ¢ä¸ºåç¨‹ï¼‰
    task = asyncio.create_task(asyncio.to_thread(task_func, *args, **kwargs))
    
    # åœ¨ä»»åŠ¡æ‰§è¡ŒæœŸé—´å®šæœŸå‘é€å¿ƒè·³
    while not task.done():
        await asyncio.sleep(1)  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡
        elapsed = time.time() - last_heartbeat
        
        # å¦‚æœè¶…è¿‡å¿ƒè·³é—´éš”ï¼Œå‘é€å¿ƒè·³æ•°æ®
        if elapsed >= heartbeat_interval:
            yield format_sse_data(" ")  # å‘é€ä¸€ä¸ªç©ºæ ¼ä½œä¸ºå¿ƒè·³
            last_heartbeat = time.time()
        
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å®Œæˆï¼ˆåœ¨å‘é€å¿ƒè·³åæ£€æŸ¥ï¼Œé¿å…åœ¨å¿ƒè·³æ£€æŸ¥ä¹‹é—´å®Œæˆæ—¶é—æ¼ï¼‰
        if task.done():
            break
    
    # ç­‰å¾…ä»»åŠ¡å®Œæˆå¹¶è¿”å›ç»“æœ
    try:
        result = await task
        # ä½¿ç”¨ç‰¹æ®Šæ ‡è®°æ¥åŒºåˆ†ç»“æœå’Œå¿ƒè·³æ•°æ®
        # è¿”å›ä¸€ä¸ªå…ƒç»„ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯æ ‡è®°ï¼Œç¬¬äºŒä¸ªå…ƒç´ æ˜¯ç»“æœ
        yield ("RESULT", result)
    except Exception as e:
        # å¦‚æœä»»åŠ¡å¤±è´¥ï¼Œè®°å½•é”™è¯¯å¹¶é‡æ–°æŠ›å‡ºå¼‚å¸¸
        print(f"âš ï¸  ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        print(traceback.format_exc())
        raise e


async def _generate_ideation_internal(query: str) -> AsyncGenerator[str, None]:
    """å†…éƒ¨ç”Ÿæˆå™¨å‡½æ•°ï¼Œæ‰§è¡Œå®é™…çš„ç”Ÿæˆé€»è¾‘"""
    # å…ˆæ£€æµ‹è¯­è¨€ï¼Œç”¨äºåç»­æ¶ˆæ¯æ¨¡æ¿
    language = await asyncio.to_thread(IdeaGenerator.detect_language, query)
    
    # æ ¹æ®è¯­è¨€è®¾ç½®æ¶ˆæ¯æ¨¡æ¿
    if language == 'zh':
        msg_templates = {
            'step1': "### ğŸ“ æ­¥éª¤ 1/9: å…³é”®è¯æå–\n\nâœ… å·²å®Œæˆ\n\n",
            'step2': "### ğŸ” æ­¥éª¤ 2/9: èƒŒæ™¯æ‰©å±•\n\nâœ… å·²å®Œæˆ\n\n",
            'step3': lambda n: f"### ğŸ“š æ­¥éª¤ 3/9: è®ºæ–‡æ£€ç´¢\n\nâœ… å·²æ£€ç´¢åˆ° {n} ç¯‡ç›¸å…³è®ºæ–‡\n\n",
            'step4': "### ğŸ’¡ æ­¥éª¤ 4/9: å¤´è„‘é£æš´\n\nâœ… å·²å®Œæˆ\n\n",
            'step5': "### âœ¨ æ­¥éª¤ 5/9: å¤šæºçµæ„Ÿç”Ÿæˆ\n\nâœ… å·²å®Œæˆ\n\n",
            'step6': lambda n: f"### ğŸ¯ æ­¥éª¤ 6/9: åˆå§‹Ideaç”Ÿæˆ\n\nâœ… å·²ç”Ÿæˆ {n} ä¸ªåˆå§‹Idea\n\n",
            'step7': lambda n: f"### ğŸ”§ æ­¥éª¤ 7/9: Ideaä¼˜åŒ–\n\nâœ… å·²ä¼˜åŒ– {n} ä¸ªIdea\n\n",
            'step8_title': "### â­ æ­¥éª¤ 8/9: æœ€ä¼˜Ideaç­›é€‰\n\n",
            'step8_best': "**æœ€ä¼˜Idea**:\n\n",
            'step8_score': "**è¯„ä¼°å¾—åˆ†**:\n\n",
            'step8_feasibility': "å¯è¡Œæ€§",
            'step8_novelty': "åˆ›æ–°æ€§",
            'step8_total': "æ€»åˆ†",
            'step9': "### ğŸ“‹ æ­¥éª¤ 9/9: ç ”ç©¶è®¡åˆ’ç”Ÿæˆ\n\n",
            'final_title': "## ğŸ“„ ç ”ç©¶è®¡åˆ’\n\n",
            'error_no_papers': "## âŒ é”™è¯¯\n\næœªæ£€ç´¢åˆ°ç›¸å…³è®ºæ–‡ï¼Œç¨‹åºç»ˆæ­¢\n\n",
            'error_no_ideas': "## âŒ é”™è¯¯\n\næœªç”Ÿæˆä»»ä½•Ideaï¼Œç¨‹åºç»ˆæ­¢\n\n",
            'error_config': "## âŒ é”™è¯¯\n\né…ç½®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡è®¾ç½®\n\n",
            'error_config_exception': lambda e: f"## âŒ é”™è¯¯\n\né…ç½®éªŒè¯å¼‚å¸¸: {e}\n\n",
            'error_llm_init': lambda e: f"## âŒ é”™è¯¯\n\nLLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}\n\n",
            'error_retriever_init': lambda e: f"## âŒ é”™è¯¯\n\nè®ºæ–‡æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {e}\n\n",
            'error_timeout': lambda t: f"## âŒ è¶…æ—¶é”™è¯¯\n\nè¯·æ±‚å¤„ç†è¶…è¿‡ {t} ç§’ï¼Œå·²è‡ªåŠ¨ç»ˆæ­¢\n\n",
            'error_general': lambda e: f"## âŒ é”™è¯¯\n\nç¨‹åºæ‰§è¡Œå¤±è´¥: {e}\n\n"
        }
    else:
        msg_templates = {
            'step1': "### ğŸ“ Step 1/9: Keyword Extraction\n\nâœ… Completed\n\n",
            'step2': "### ğŸ” Step 2/9: Background Expansion\n\nâœ… Completed\n\n",
            'step3': lambda n: f"### ğŸ“š Step 3/9: Paper Retrieval\n\nâœ… Retrieved {n} related papers\n\n",
            'step4': "### ğŸ’¡ Step 4/9: Brainstorming\n\nâœ… Completed\n\n",
            'step5': "### âœ¨ Step 5/9: Multi-source Inspiration Generation\n\nâœ… Completed\n\n",
            'step6': lambda n: f"### ğŸ¯ Step 6/9: Initial Idea Generation\n\nâœ… Generated {n} initial ideas\n\n",
            'step7': lambda n: f"### ğŸ”§ Step 7/9: Idea Refinement\n\nâœ… Refined {n} ideas\n\n",
            'step8_title': "### â­ Step 8/9: Best Idea Selection\n\n",
            'step8_best': "**Best Idea**:\n\n",
            'step8_score': "**Evaluation Score**:\n\n",
            'step8_feasibility': "Feasibility",
            'step8_novelty': "Novelty",
            'step8_total': "Total",
            'step9': "### ğŸ“‹ Step 9/9: Research Plan Generation\n\n",
            'final_title': "## ğŸ“„ Research Plan\n\n",
            'error_no_papers': "## âŒ Error\n\nNo related papers found. Process terminated.\n\n",
            'error_no_ideas': "## âŒ Error\n\nNo ideas generated. Process terminated.\n\n",
            'error_config': "## âŒ Error\n\nConfiguration validation failed. Please check environment variables.\n\n",
            'error_config_exception': lambda e: f"## âŒ Error\n\nConfiguration validation exception: {e}\n\n",
            'error_llm_init': lambda e: f"## âŒ Error\n\nLLM client initialization failed: {e}\n\n",
            'error_retriever_init': lambda e: f"## âŒ Error\n\nPaper retriever initialization failed: {e}\n\n",
            'error_timeout': lambda t: f"## âŒ Timeout Error\n\nRequest processing exceeded {t} seconds. Automatically terminated.\n\n",
            'error_general': lambda e: f"## âŒ Error\n\nProcess execution failed: {e}\n\n"
        }
    
    # éªŒè¯é…ç½®ï¼ˆä¸è¾“å‡ºï¼‰
    try:
        config_valid = await asyncio.to_thread(Config.validate_config)
        if not config_valid:
            for chunk in stream_message(msg_templates['error_config']):
                yield chunk
            return
    except Exception as e:
        for chunk in stream_message(msg_templates['error_config_exception'](e)):
            yield chunk
        return
    
    # åˆ›å»ºç»„ä»¶ï¼ˆä¸è¾“å‡ºåˆå§‹åŒ–ä¿¡æ¯ï¼‰
    try:
        client = LLMClient()
    except Exception as e:
        for chunk in stream_message(msg_templates['error_llm_init'](e)):
            yield chunk
        return
    
    try:
        retriever = PaperRetriever()
    except Exception as e:
        for chunk in stream_message(msg_templates['error_retriever_init'](e)):
            yield chunk
        return
    generator = IdeaGenerator(client, language=language)
    
    # æ­¥éª¤1: æå–å…³é”®è¯ï¼ˆç®€åŒ–è¾“å‡ºï¼‰
    keywords = await asyncio.to_thread(generator.extract_keywords, query)
    for chunk in stream_message(msg_templates['step1']):
        yield chunk
    
    # æ­¥éª¤2: æ‰©å±•èƒŒæ™¯ï¼ˆç®€åŒ–è¾“å‡ºï¼‰
    expanded_background = await asyncio.to_thread(generator.expand_background, query, keywords)
    for chunk in stream_message(msg_templates['step2']):
        yield chunk
    
    # æ­¥éª¤3: æ··åˆæ£€ç´¢è®ºæ–‡ï¼ˆç®€åŒ–è¾“å‡ºï¼‰
    papers = await asyncio.to_thread(retriever.hybrid_retrieve, expanded_background, keywords)
    for chunk in stream_message(msg_templates['step3'](len(papers))):
        yield chunk
    
    if not papers:
        for chunk in stream_message(msg_templates['error_no_papers']):
            yield chunk
        return
    
    # æ­¥éª¤4: Brainstormï¼ˆç®€åŒ–è¾“å‡ºï¼‰
    brainstorm = await asyncio.to_thread(generator.generate_brainstorm, expanded_background)
    for chunk in stream_message(msg_templates['step4']):
        yield chunk
    
    # æ­¥éª¤5: å¤šæºInspirationï¼ˆç®€åŒ–è¾“å‡ºï¼‰
    inspirations = await asyncio.to_thread(
        generator.generate_multi_inspirations,
        expanded_background, query, papers
    )
    for chunk in stream_message(msg_templates['step5']):
        yield chunk
    
    # æ­¥éª¤6: ç”ŸæˆIdeaï¼ˆç®€åŒ–è¾“å‡ºï¼‰
    initial_ideas = await asyncio.to_thread(
        generator.generate_ideas,
        expanded_background, inspirations, brainstorm, query
    )
    for chunk in stream_message(msg_templates['step6'](len(initial_ideas))):
        yield chunk
    
    if not initial_ideas:
        for chunk in stream_message(msg_templates['error_no_ideas']):
            yield chunk
        return
    
    # æ­¥éª¤7: è¿­ä»£ä¼˜åŒ–ï¼ˆç®€åŒ–è¾“å‡ºï¼‰
    # å…ˆå‘é€æ­¥éª¤æ ‡é¢˜å’Œè¿›åº¦æç¤ºï¼Œè®©å®¢æˆ·ç«¯çŸ¥é“æœåŠ¡ç«¯è¿˜åœ¨å·¥ä½œ
    if language == 'zh':
        step7_title = "### ğŸ”§ æ­¥éª¤ 7/9: Ideaä¼˜åŒ–\n\n"
        step7_progress = "ğŸ”„ æ­£åœ¨ä¼˜åŒ–ä¸­ï¼Œè¯·ç¨å€™...\n\n"
    else:
        step7_title = "### ğŸ”§ Step 7/9: Idea Refinement\n\n"
        step7_progress = "ğŸ”„ Refining ideas, please wait...\n\n"
    
    for chunk in stream_message(step7_title):
        yield chunk
    for chunk in stream_message(step7_progress):
        yield chunk
    
    # æ‰§è¡Œä»»åŠ¡å¹¶å‘é€å¿ƒè·³
    refined_ideas = None
    async for item in run_with_heartbeat(
        generator.iterative_refine_ideas,
        expanded_background, papers, initial_ideas,
        heartbeat_interval=25  # æ¯25ç§’å‘é€ä¸€æ¬¡å¿ƒè·³
    ):
        if isinstance(item, tuple) and len(item) == 2 and item[0] == "RESULT":  # ä»»åŠ¡å®Œæˆï¼Œè¿”å›ç»“æœ
            refined_ideas = item[1]
            break
        else:  # å¿ƒè·³æ•°æ®
            yield item
    
    # å‘é€å®Œæˆæ¶ˆæ¯
    for chunk in stream_message(msg_templates['step7'](len(refined_ideas))):
        yield chunk
    
    # æ­¥éª¤8: è¯„ä¼°ç­›é€‰ï¼ˆä¿ç•™å…³é”®ä¿¡æ¯ï¼Œä½†ç®€åŒ–æ ¼å¼ï¼‰
    # å…ˆå‘é€æ­¥éª¤æ ‡é¢˜å’Œè¿›åº¦æç¤º
    for chunk in stream_message(msg_templates['step8_title']):
        yield chunk
    
    if language == 'zh':
        step8_progress = "ğŸ”„ æ­£åœ¨è¯„ä¼°ä¸­ï¼Œè¯·ç¨å€™...\n\n"
    else:
        step8_progress = "ğŸ”„ Evaluating ideas, please wait...\n\n"
    
    for chunk in stream_message(step8_progress):
        yield chunk
    
    # æ‰§è¡Œä»»åŠ¡å¹¶å‘é€å¿ƒè·³
    best_idea = None
    score = None
    async for item in run_with_heartbeat(
        generator.evaluate_and_select_best_idea,
        expanded_background, refined_ideas,
        heartbeat_interval=25  # æ¯25ç§’å‘é€ä¸€æ¬¡å¿ƒè·³
    ):
        if isinstance(item, tuple) and len(item) == 2 and item[0] == "RESULT":  # ä»»åŠ¡å®Œæˆï¼Œè¿”å›ç»“æœ
            best_idea, score = item[1]  # item[1]æ˜¯(best_idea, score)å…ƒç»„
            break
        else:  # å¿ƒè·³æ•°æ®
            yield item
    
    # åªè¾“å‡ºè¯„ä¼°å¾—åˆ†ï¼Œä¸è¾“å‡ºæœ€ä¼˜ideaçš„å…·ä½“å†…å®¹
    for chunk in stream_message(f"{msg_templates['step8_score']}- {msg_templates['step8_feasibility']}: {score['feasibility']:.2f}/5.0\n- {msg_templates['step8_novelty']}: {score['novelty']:.2f}/5.0\n- {msg_templates['step8_total']}: {score['total']:.2f}/10.0\n\n"):
        yield chunk
    
    # æ­¥éª¤9: ç”Ÿæˆç ”ç©¶è®¡åˆ’ï¼ˆå®Œæ•´è¾“å‡ºï¼‰
    for chunk in stream_message(msg_templates['step9']):
        yield chunk
    
    if language == 'zh':
        step9_progress = "ğŸ”„ æ­£åœ¨ç”Ÿæˆç ”ç©¶è®¡åˆ’ï¼Œè¯·ç¨å€™...\n\n"
    else:
        step9_progress = "ğŸ”„ Generating research plan, please wait...\n\n"
    
    for chunk in stream_message(step9_progress):
        yield chunk
    
    # æ‰§è¡Œä»»åŠ¡å¹¶å‘é€å¿ƒè·³
    research_plan = None
    try:
        async for item in run_with_heartbeat(
            generator.generate_research_plan,
            query, papers, best_idea, inspirations["global_inspiration"],
            heartbeat_interval=25  # æ¯25ç§’å‘é€ä¸€æ¬¡å¿ƒè·³
        ):
            if isinstance(item, tuple) and len(item) == 2 and item[0] == "RESULT":  # ä»»åŠ¡å®Œæˆï¼Œè¿”å›ç»“æœ
                research_plan = item[1]
                print(f"[DEBUG] ç ”ç©¶è®¡åˆ’ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(research_plan) if research_plan else 0}")
                break
            else:  # å¿ƒè·³æ•°æ®
                yield item
    except Exception as e:
        print(f"[DEBUG] ç ”ç©¶è®¡åˆ’ç”Ÿæˆå¼‚å¸¸: {e}")
        import traceback
        print(traceback.format_exc())
        if language == 'zh':
            error_msg = f"âš ï¸ ç ”ç©¶è®¡åˆ’ç”Ÿæˆå¤±è´¥: {str(e)}\n\n"
        else:
            error_msg = f"âš ï¸ Research plan generation failed: {str(e)}\n\n"
        for chunk in stream_message(error_msg):
            yield chunk
        return
    
    # æ£€æŸ¥ç ”ç©¶è®¡åˆ’æ˜¯å¦ä¸ºç©º
    if not research_plan or research_plan.strip() == "":
        print(f"[DEBUG] ç ”ç©¶è®¡åˆ’ä¸ºç©º: research_plan={research_plan}")
        if language == 'zh':
            error_msg = "âš ï¸ ç ”ç©¶è®¡åˆ’ç”Ÿæˆå¤±è´¥ï¼Œè¿”å›ç©ºå†…å®¹\n\n"
        else:
            error_msg = "âš ï¸ Research plan generation failed, returned empty content\n\n"
        for chunk in stream_message(error_msg):
            yield chunk
    else:
        # æ·»åŠ åˆ†éš”çº¿å’Œæ ‡é¢˜ï¼Œä¼˜åŒ–æ ¼å¼
        for chunk in stream_message("---\n\n"):
            yield chunk
        for chunk in stream_message(msg_templates['final_title']):
            yield chunk
        # å®Œæ•´è¾“å‡ºç ”ç©¶è®¡åˆ’
        for chunk in stream_message(f"{research_plan}\n\n"):
            yield chunk


async def generate_ideation_stream(query: str) -> AsyncGenerator[str, None]:
    """ç”ŸæˆIdeaçš„æµå¼è¾“å‡ºç”Ÿæˆå™¨ï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼Œå…¼å®¹Python 3.9ï¼‰"""
    start_time = time.time()
    
    try:
        # æ‰§è¡Œç”Ÿæˆé€»è¾‘ï¼Œåœ¨æ¯æ¬¡ yield å‰æ£€æŸ¥è¶…æ—¶
        async for item in _generate_ideation_internal(query):
            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            elapsed = time.time() - start_time
            if elapsed > REQUEST_TIMEOUT:
                # æ£€æµ‹è¯­è¨€ä»¥ä½¿ç”¨æ­£ç¡®çš„é”™è¯¯æ¶ˆæ¯
                language = await asyncio.to_thread(IdeaGenerator.detect_language, query)
                if language == 'zh':
                    timeout_msg = f"## âŒ è¶…æ—¶é”™è¯¯\n\nè¯·æ±‚å¤„ç†è¶…è¿‡ {REQUEST_TIMEOUT} ç§’ï¼Œå·²è‡ªåŠ¨ç»ˆæ­¢\n\n"
                else:
                    timeout_msg = f"## âŒ Timeout Error\n\nRequest processing exceeded {REQUEST_TIMEOUT} seconds. Automatically terminated.\n\n"
                for chunk in stream_message(timeout_msg):
                    yield chunk
                yield format_sse_done()
                return
            yield item
        
        # å‘é€ç»“æŸæ ‡è®°
        yield format_sse_done()
                
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        print(f"âŒ ç”Ÿæˆå™¨é”™è¯¯: {e}\n{error_trace}")
        # æ£€æµ‹è¯­è¨€ä»¥ä½¿ç”¨æ­£ç¡®çš„é”™è¯¯æ¶ˆæ¯
        try:
            language = await asyncio.to_thread(IdeaGenerator.detect_language, query)
            if language == 'zh':
                error_msg = f"## âŒ é”™è¯¯\n\nç¨‹åºæ‰§è¡Œå¤±è´¥: {e}\n\n```\n{error_trace}\n```\n\n"
            else:
                error_msg = f"## âŒ Error\n\nProcess execution failed: {e}\n\n```\n{error_trace}\n```\n\n"
        except:
            # å¦‚æœæ£€æµ‹è¯­è¨€å¤±è´¥ï¼Œä½¿ç”¨è‹±æ–‡
            error_msg = f"## âŒ Error\n\nProcess execution failed: {e}\n\n```\n{error_trace}\n```\n\n"
        for chunk in stream_message(error_msg):
            yield chunk
        yield format_sse_done()


@app.post("/ideation")
async def ideation(request: IdeationRequest):
    """
    Ideaç”ŸæˆAPIç«¯ç‚¹
    """
    if not request.query or not request.query.strip():
        raise HTTPException(status_code=400, detail="Query cannot be empty")
    
    return StreamingResponse(
        generate_ideation_stream(request.query),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
            "Access-Control-Allow-Origin": "*"  # æ˜ç¡®å…è®¸SSEè·¨åŸŸ
        }
    )


@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹ - è½»é‡çº§å“åº”"""
    return {"status": "ok", "service": "ICAIS2025-Ideation API", "timestamp": time.time()}


@app.get("/")
async def root():
    """æ ¹ç«¯ç‚¹"""
    return {
        "service": "ICAIS2025-Ideation API",
        "version": "1.0.0",
        "health": "http://localhost:3000/health",
        "docs": "http://localhost:3000/docs",
        "ideation": "POST /ideation"
    }


# ä¼˜é›…å…³é—­å¤„ç†
def shutdown_handler(signum, frame):
    print(f"\nâš ï¸ æ”¶åˆ°ç»ˆæ­¢ä¿¡å· {signum}ï¼Œæ­£åœ¨å…³é—­æœåŠ¡...")
    sys.exit(0)


signal.signal(signal.SIGINT, shutdown_handler)
signal.signal(signal.SIGTERM, shutdown_handler)

if __name__ == "__main__":
    import uvicorn
    
    # éªŒè¯ç«¯å£å¯ç”¨æ€§
    import socket
    def check_port(port):
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            try:
                s.bind(("0.0.0.0", port))
                return True
            except OSError:
                return False
    
    if not check_port(3000):
        print(f"âŒ ç«¯å£3000å·²è¢«å ç”¨ï¼Œè¯·æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–æœåŠ¡åœ¨ä½¿ç”¨")
        sys.exit(1)
    
    print("ğŸš€ å¯åŠ¨ FastAPI æœåŠ¡...")
    print(f"ğŸ“ ç›‘å¬åœ°å€: http://0.0.0.0:3000")
    print(f"ğŸ“ å¥åº·æ£€æŸ¥: curl http://localhost:3000/health")
    print(f"ğŸ“š APIæ–‡æ¡£: http://localhost:3000/docs")
    
    # ä½¿ç”¨æ›´å¥å£®çš„uvicorné…ç½®
    uvicorn.run(
        app,  # ç›´æ¥ä¼ é€’appå¯¹è±¡ï¼Œå› ä¸ºappåœ¨å½“å‰æ¨¡å—ä¸­å®šä¹‰
        host="0.0.0.0",
        port=3000,
        log_level="info",
        access_log=True,
        reload=False,  # ç”Ÿäº§ç¯å¢ƒå…³é—­çƒ­é‡è½½
        workers=1,  # å•workeré¿å…å¹¶å‘é—®é¢˜
        loop="asyncio",  # æ˜ç¡®ä½¿ç”¨asyncioå¾ªç¯
        timeout_keep_alive=30,  # keep-aliveè¶…æ—¶
        limit_concurrency=100,  # é™åˆ¶å¹¶å‘æ•°
    )
