import requests
import time
import numpy as np
from typing import List, Dict, Optional
from config import Config
from embedding_client import EmbeddingClient


class PaperRetriever:
    """è®ºæ–‡æ£€ç´¢å™¨ - åŸºäºSemantic Scholar API"""

    def __init__(self):
        self.config = Config
        self.embedding_client = None
        self._init_embedding_client()

    def _init_embedding_client(self):
        """åˆå§‹åŒ–embeddingå®¢æˆ·ç«¯"""
        try:
            print(f"ğŸ”„ æ­£åœ¨åˆå§‹åŒ–Embeddingå®¢æˆ·ç«¯: {self.config.EMBEDDING_MODEL_NAME}...")
            self.embedding_client = EmbeddingClient()
            print(f"âœ… Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸  Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†è·³è¿‡è¯­ä¹‰é‡æ’åº")
            self.embedding_client = None

    def get_newest_paper(self, query: str, max_results: Optional[int] = None, max_retries: Optional[int] = None) -> List[Dict]:
        """è·å–æœ€æ–°è®ºæ–‡"""
        max_results = max_results or self.config.MAX_PAPERS_PER_QUERY
        max_retries = max_retries or self.config.SEMANTIC_SCHOLAR_MAX_RETRIES

        url = "http://api.semanticscholar.org/graph/v1/paper/search/bulk"
        params = {"query": query, "fields": "title,abstract,paperId", "sort": "publicationDate:desc"}

        for attempt in range(max_retries):
            try:
                response = requests.get(url, params=params, timeout=self.config.SEMANTIC_SCHOLAR_TIMEOUT)
                data = response.json()

                if 'data' in data:
                    papers = data['data'][:max_results] if data['data'] else []
                    if papers:
                        return papers
                    # å¦‚æœè¿”å›ç©ºåˆ—è¡¨ï¼Œç»§ç»­é‡è¯•
                    if attempt < max_retries - 1:
                        wait_time = min(2 ** attempt, 5)  # æŒ‡æ•°é€€é¿ï¼Œæœ€å¤š5ç§’
                        print(f"è·å–æœ€æ–°è®ºæ–‡è¿”å›ç©ºæ•°æ®ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                    continue
                else:
                    # å“åº”ä¸­æ²¡æœ‰'data'å­—æ®µï¼Œç»§ç»­é‡è¯•
                    if attempt < max_retries - 1:
                        wait_time = min(2 ** attempt, 5)
                        print(f"è·å–æœ€æ–°è®ºæ–‡å“åº”æ ¼å¼å¼‚å¸¸ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                    continue
            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    wait_time = min(2 ** attempt, 5)  # æŒ‡æ•°é€€é¿ï¼Œæœ€å¤š5ç§’
                    print(f"è·å–æœ€æ–°è®ºæ–‡è¶…æ—¶ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"è·å–æœ€æ–°è®ºæ–‡æœ€ç»ˆå¤±è´¥: è¶…æ—¶")
                    return []
            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    wait_time = min(2 ** attempt, 5)  # æŒ‡æ•°é€€é¿
                    print(f"è·å–æœ€æ–°è®ºæ–‡å¤±è´¥: {e}ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"è·å–æœ€æ–°è®ºæ–‡æœ€ç»ˆå¤±è´¥: {e}")
                    return []
            except Exception as e:
                if attempt < max_retries - 1:
                    wait_time = min(2 ** attempt, 5)
                    print(f"è·å–æœ€æ–°è®ºæ–‡å¤±è´¥: {e}ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"è·å–æœ€æ–°è®ºæ–‡æœ€ç»ˆå¤±è´¥: {e}")
                    return []

        return []

    def get_highly_cited_paper(self, query: str, max_results: Optional[int] = None, max_retries: Optional[int] = None) -> List[Dict]:
        """è·å–é«˜å¼•ç”¨è®ºæ–‡"""
        max_results = max_results or self.config.MAX_PAPERS_PER_QUERY
        max_retries = max_retries or self.config.SEMANTIC_SCHOLAR_MAX_RETRIES

        url = "http://api.semanticscholar.org/graph/v1/paper/search/bulk"
        params = {"query": query, "fields": "title,abstract,paperId", "sort": "citationCount:desc"}

        for attempt in range(max_retries):
            try:
                response = requests.get(url, params=params, timeout=self.config.SEMANTIC_SCHOLAR_TIMEOUT)
                data = response.json()

                if 'data' in data:
                    papers = data['data'][:max_results] if data['data'] else []
                    if papers:
                        return papers
                    # å¦‚æœè¿”å›ç©ºåˆ—è¡¨ï¼Œç»§ç»­é‡è¯•
                    if attempt < max_retries - 1:
                        wait_time = min(2 ** attempt, 5)  # æŒ‡æ•°é€€é¿ï¼Œæœ€å¤š5ç§’
                        print(f"è·å–é«˜å¼•ç”¨è®ºæ–‡è¿”å›ç©ºæ•°æ®ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                    continue
                else:
                    # å“åº”ä¸­æ²¡æœ‰'data'å­—æ®µï¼Œç»§ç»­é‡è¯•
                    if attempt < max_retries - 1:
                        wait_time = min(2 ** attempt, 5)
                        print(f"è·å–é«˜å¼•ç”¨è®ºæ–‡å“åº”æ ¼å¼å¼‚å¸¸ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                    continue
            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    wait_time = min(2 ** attempt, 5)  # æŒ‡æ•°é€€é¿ï¼Œæœ€å¤š5ç§’
                    print(f"è·å–é«˜å¼•ç”¨è®ºæ–‡è¶…æ—¶ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"è·å–é«˜å¼•ç”¨è®ºæ–‡æœ€ç»ˆå¤±è´¥: è¶…æ—¶")
                    return []
            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    wait_time = min(2 ** attempt, 5)  # æŒ‡æ•°é€€é¿
                    print(f"è·å–é«˜å¼•ç”¨è®ºæ–‡å¤±è´¥: {e}ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"è·å–é«˜å¼•ç”¨è®ºæ–‡æœ€ç»ˆå¤±è´¥: {e}")
                    return []
            except Exception as e:
                if attempt < max_retries - 1:
                    wait_time = min(2 ** attempt, 5)
                    print(f"è·å–é«˜å¼•ç”¨è®ºæ–‡å¤±è´¥: {e}ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"è·å–é«˜å¼•ç”¨è®ºæ–‡æœ€ç»ˆå¤±è´¥: {e}")
                    return []

        return []

    def get_relevant_paper(self, query: str, max_results: Optional[int] = None, max_retries: Optional[int] = None) -> List[Dict]:
        """è·å–ç›¸å…³è®ºæ–‡"""
        max_results = max_results or self.config.MAX_PAPERS_PER_QUERY
        max_retries = max_retries or self.config.SEMANTIC_SCHOLAR_MAX_RETRIES

        url = "http://api.semanticscholar.org/graph/v1/paper/search"
        params = {"query": query, "fields": "title,abstract,paperId"}

        for attempt in range(max_retries):
            try:
                response = requests.get(url, params=params, timeout=self.config.SEMANTIC_SCHOLAR_TIMEOUT)
                data = response.json()

                if 'data' in data:
                    papers = data['data'][:max_results] if data['data'] else []
                    if papers:
                        return papers
                    # å¦‚æœè¿”å›ç©ºåˆ—è¡¨ï¼Œç»§ç»­é‡è¯•
                    if attempt < max_retries - 1:
                        wait_time = min(2 ** attempt, 5)  # æŒ‡æ•°é€€é¿ï¼Œæœ€å¤š5ç§’
                        print(f"è·å–ç›¸å…³è®ºæ–‡è¿”å›ç©ºæ•°æ®ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                    continue
                else:
                    # å“åº”ä¸­æ²¡æœ‰'data'å­—æ®µï¼Œç»§ç»­é‡è¯•
                    if attempt < max_retries - 1:
                        wait_time = min(2 ** attempt, 5)
                        print(f"è·å–ç›¸å…³è®ºæ–‡å“åº”æ ¼å¼å¼‚å¸¸ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                    continue
            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    wait_time = min(2 ** attempt, 5)  # æŒ‡æ•°é€€é¿ï¼Œæœ€å¤š5ç§’
                    print(f"è·å–ç›¸å…³è®ºæ–‡è¶…æ—¶ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"è·å–ç›¸å…³è®ºæ–‡æœ€ç»ˆå¤±è´¥: è¶…æ—¶")
                    return []
            except requests.exceptions.RequestException as e:
                if attempt < max_retries - 1:
                    wait_time = min(2 ** attempt, 5)  # æŒ‡æ•°é€€é¿
                    print(f"è·å–ç›¸å…³è®ºæ–‡å¤±è´¥: {e}ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"è·å–ç›¸å…³è®ºæ–‡æœ€ç»ˆå¤±è´¥: {e}")
                    return []
            except Exception as e:
                if attempt < max_retries - 1:
                    wait_time = min(2 ** attempt, 5)
                    print(f"è·å–ç›¸å…³è®ºæ–‡å¤±è´¥: {e}ï¼Œ{wait_time}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"è·å–ç›¸å…³è®ºæ–‡æœ€ç»ˆå¤±è´¥: {e}")
                    return []

        return []

    def merge_and_deduplicate(self, results: Dict[str, List[Dict]]) -> List[Dict]:
        """èåˆå’Œå»é‡è®ºæ–‡"""
        seen_ids = set()
        all_papers = []

        for paper_list in results.values():
            for paper in paper_list:
                paper_id = paper.get('paperId') or paper.get('title', '')
                if paper_id and paper_id not in seen_ids:
                    seen_ids.add(paper_id)
                    all_papers.append(paper)

        return all_papers

    def rerank_by_similarity(self, papers: List[Dict], background_embedding: np.ndarray, background_text: str) -> List[Dict]:
        """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦é‡æ’åºè®ºæ–‡"""
        if not self.embedding_client or len(papers) == 0:
            return papers

        try:
            # ä¸ºæ¯ç¯‡è®ºæ–‡è®¡ç®—embedding
            paper_texts = []
            for paper in papers:
                abstract = paper.get('abstract', '') or ''
                title = paper.get('title', '') or ''
                text = f"{title} {abstract}".strip()
                paper_texts.append(text if text else " ")

            # æ‰¹é‡è®¡ç®—embeddingï¼ˆé€šè¿‡APIï¼‰
            paper_embeddings = self.embedding_client.encode(paper_texts, show_progress_bar=False)
            
            # ç¡®ä¿æ˜¯2Dæ•°ç»„
            if paper_embeddings.ndim == 1:
                paper_embeddings = paper_embeddings.reshape(1, -1)

            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = []
            for paper_emb in paper_embeddings:
                similarity = np.dot(background_embedding, paper_emb) / (
                    np.linalg.norm(background_embedding) * np.linalg.norm(paper_emb) + 1e-8
                )
                similarities.append(similarity)

            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            sorted_papers = sorted(
                zip(papers, similarities),
                key=lambda x: x[1],
                reverse=True
            )

            return [paper for paper, _ in sorted_papers]

        except Exception as e:
            print(f"âš ï¸  è¯­ä¹‰é‡æ’åºå¤±è´¥: {e}ï¼Œè¿”å›åŸå§‹é¡ºåº")
            return papers

    def hybrid_retrieve(self, expanded_background: str, keywords: List[str]) -> List[Dict]:
        """
        æ··åˆæ£€ç´¢ç­–ç•¥ - ä»…ä½¿ç”¨Semantic Scholar API
        """
        # æ„é€ æŸ¥è¯¢å­—ç¬¦ä¸²
        if len(keywords) == 1:
            query = keywords[0]
        else:
            query = " | ".join(f'"{item}"' for item in keywords)

        print(f"ğŸ” æ£€ç´¢å…³é”®è¯: {query}")

        # 1. å¹¶è¡Œæ£€ç´¢ä¸‰ç±»è®ºæ–‡ï¼ˆå³ä½¿éƒ¨åˆ†å¤±è´¥ä¹Ÿç»§ç»­ï¼‰
        import concurrent.futures

        newest_papers = []
        highly_cited_papers = []
        relevant_papers = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            future_newest = executor.submit(self.get_newest_paper, query)
            future_highly_cited = executor.submit(self.get_highly_cited_paper, query)
            future_relevant = executor.submit(self.get_relevant_paper, query)

            # è·å–ç»“æœï¼Œå³ä½¿å¤±è´¥ä¹Ÿç»§ç»­
            try:
                newest_papers = future_newest.result(timeout=120)  # æœ€å¤šç­‰å¾…2åˆ†é’Ÿ
            except Exception as e:
                print(f"âš ï¸  è·å–æœ€æ–°è®ºæ–‡å¤±è´¥: {e}")
                newest_papers = []

            try:
                highly_cited_papers = future_highly_cited.result(timeout=120)
            except Exception as e:
                print(f"âš ï¸  è·å–é«˜å¼•ç”¨è®ºæ–‡å¤±è´¥: {e}")
                highly_cited_papers = []

            try:
                relevant_papers = future_relevant.result(timeout=120)
            except Exception as e:
                print(f"âš ï¸  è·å–ç›¸å…³è®ºæ–‡å¤±è´¥: {e}")
                relevant_papers = []

        # 2. èåˆå’Œå»é‡
        results = {
            "newest_papers": newest_papers or [],
            "highly_cited_papers": highly_cited_papers or [],
            "relevant_papers": relevant_papers or []
        }
        all_papers = self.merge_and_deduplicate(results)

        print(f"ğŸ“š æ£€ç´¢åˆ° {len(all_papers)} ç¯‡è®ºæ–‡ï¼ˆå»é‡åï¼‰")

        # å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°ä»»ä½•è®ºæ–‡ï¼Œè¿”å›ç©ºåˆ—è¡¨
        if not all_papers:
            print("âš ï¸  æœªæ£€ç´¢åˆ°ä»»ä½•è®ºæ–‡")
            return []

        # 3. ä½¿ç”¨embeddingå®¢æˆ·ç«¯è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦å¹¶é‡æ’åº
        if self.embedding_client:
            try:
                background_embedding = self.embedding_client.encode(expanded_background, show_progress_bar=False)
                if background_embedding is not None and len(background_embedding) > 0:
                    all_papers = self.rerank_by_similarity(all_papers, background_embedding, expanded_background)
                    print(f"âœ… è¯­ä¹‰é‡æ’åºå®Œæˆ")
                else:
                    print(f"âš ï¸  Embeddingç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡è¯­ä¹‰é‡æ’åº")
            except Exception as e:
                print(f"âš ï¸  è¯­ä¹‰é‡æ’åºå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹é¡ºåº")

        # 4. è¿”å›top-k
        return all_papers[:self.config.MAX_TOTAL_PAPERS]

